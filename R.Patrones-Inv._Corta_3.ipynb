{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "silent-taylor",
   "metadata": {},
   "source": [
    "# Natural Languaje Processing (NLP)\n",
    "\n",
    "<img src=\"Images/NLP.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-dependence",
   "metadata": {},
   "source": [
    "## Lenguaje Natural: \n",
    "\n",
    "En neuropsicología, lingüística y filosofía del lenguaje se le llama lenguaje natural a cualquier lenguaje que ha evolucionado de forma natural en los seres humanos mediante el uso y la repetición sin planificación o premeditación consciente. [1] En otras palabras es la manera en que nos comunicamos los seres humanos. Principalmente mediante el habla y la escritura. [2]\n",
    "\n",
    "<img src=\"Images/NL.png\" width=\"600\">\n",
    "\n",
    "## Procesamiento Natural del Lenguaje:\n",
    "\n",
    "El procesamiento natural del lenguaje es un área de las ciencias de la computación y de la inteligencia artificial, que se dedica a la manipulación automática del lenguaje natural. Como los son la manipulación, comprensión y generación del lenguaje natural. [2] En otras palabras, busca que las computadoras logren entender el lenguaje humano.\n",
    "\n",
    "<img src=\"Images/NLP.png\" width=\"600\">\n",
    "\n",
    "## Retos:\n",
    "\n",
    "El procesamiento natural del lenguaje es desafiante debido a la complejidad del lenguaje humano y sus muchas características. Reglas gramaticales, expresiones idiomáticas, costumbrismos, ambigüedad del vocabulario o expresiones, entre otras más.\n",
    "\n",
    "<img src=\"Images/chinese_whisper.jpeg\" width=\"600\">\n",
    "\n",
    "## Usos y aplicaciones\n",
    "\n",
    "* Tranductores. Ej: Google Translate\n",
    "* Antispam. Ej: Gmail\n",
    "* Buscadores. Ej: Google, Yahoo , etc\n",
    "* Resumen automático de texto.\n",
    "* Corrección gramatical.\n",
    "* Chatbots.\n",
    "* Asistentes virtuales. Ej: Siri, Google Voice, Alexa\n",
    "* Extraer información para categorizar un texto.\n",
    "* Publicidad (*Advertisement matching*)\n",
    "* Simplificar texto.\n",
    "* Resumir un texto.\n",
    "* Reconocimiento del habla.\n",
    "* Extraer el estado del ánimo en base al texto. (*sentimental analysis*)\n",
    "* Generar una descripción de una imagen o un video.\n",
    "* etc.\n",
    "\n",
    "## ¿Cómo comprenden las computadoras el lenguaje humano?\n",
    "\n",
    "* Primero se necesita alimentar a la máquina con suficiente datos para que la máquina logre aprender\n",
    "* Posteriormente la máquina creará un vector con representación numérica de las palabras.\n",
    "* Usando algoritmos de aprendizaje automatizado se procesa los vectores de entrada.\n",
    "* Para que finalmente la máquina logrará responder en un lenguaje humano.\n",
    "\n",
    "### Proceso morfológico\n",
    "\n",
    "Busca separar el texto en párrafos, oraciones y palabras.\n",
    "\n",
    "### Análisis sintáctico\n",
    "\n",
    "Busca verificar que el texto este bien formado mediante el léxico y la gramática. Para separar en una estructura que muestre las relaciones sintácticas entre las palabras.\n",
    "\n",
    "### Análisis semántico\n",
    "\n",
    "Este análisis busca obtener el significado del texto. Primero analizando el significado individual de cada palabra (lexical semantics). Luego analizando el significado de la combinación de palabras en contexto. [3] Busca verificar el significado del texto mediante diccionarios.\n",
    "\n",
    "* Sentido de la palabra: Identificar el sentido con el que se usa la palabra en el contexto de la oración.\n",
    "* Extracción de relaciones: tratar de entender la relación entre las palabras.\n",
    "\n",
    "### Análisis Pragmático\n",
    "\n",
    "Busca encontrar el contexto real del texto mediante referencias previas obtenidas del análisis semántico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-connecticut",
   "metadata": {},
   "source": [
    "## Componentes del procesamiento natural del lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-presentation",
   "metadata": {},
   "source": [
    "## Ejemplo 1: Pre-procesado de texto mediante la librería NLTK (Natural Language Toolkit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "satellite-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"MacGyver resolved the problem using only a clip and some rubber bands.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-console",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "\n",
    "Toma una oración y la divide en pequeñas porciones o **Tokens**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceramic-spirit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MacGyver', 'resolved', 'the', 'problem', 'using', 'only', 'a', 'clip', 'and', 'some', 'rubber', 'bands', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-privacy",
   "metadata": {},
   "source": [
    "### Stemming & Lemmatization\n",
    "\n",
    "Se busca normalizar las palabras en su forma base.\n",
    "\n",
    "#### Stemming\n",
    "\n",
    "Remueve y reemplaza sufijos.\n",
    "\n",
    "* Porter \n",
    "* Lancaster\n",
    "* Snowball (Multi-language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "painful-coalition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['macgyv', 'resolv', 'the', 'problem', 'use', 'onli', 'a', 'clip', 'and', 'some', 'rubber', 'band', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer()\n",
    "tokens_stem_1 = [word_stemmer.stem(word) for word in tokens]\n",
    "print(tokens_stem_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-service",
   "metadata": {},
   "source": [
    "Toma una expresión y remueve el prefijo o sufijo que encagen en la expresión.\n",
    "\n",
    "* Regular Expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "higher-trade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MacGyver', 'resolved', 'the', 'problem', 'us', 'only', 'a', 'clip', 'and', 'some', 'rubber', 'bands', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "word_stemmer = RegexpStemmer('ing')\n",
    "tokens_stem_2 = [word_stemmer.stem(word) for word in tokens]\n",
    "print(tokens_stem_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-tattoo",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "Convierte las palabras a su forma básica llamada lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "promotional-brass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MacGyver', 'resolve', 'the', 'problem', 'use', 'only', 'a', 'clip', 'and', 'some', 'rubber', 'band', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmas = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for token in tokens:\n",
    "    lemma = lemmatizer.lemmatize(token,pos='n')\n",
    "    lemma = lemmatizer.lemmatize(lemma,pos='v')\n",
    "    lemmas.append(lemma)\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-stake",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Son palabras que no aportan mucho al análisis del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "intermediate-ferry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mustn', 'who', 'how', \"don't\", 'your', 'you', 'hers', \"weren't\", 'before', 'until', 'o', 'a', 'about', 'our', \"you've\", \"mustn't\", 'had', 'wouldn', 'yours', 'him', 'did', 'each', 'me', 'be', 'but', 'these', 'ain', 'they', 'by', 'and', 'them', \"she's\", \"hadn't\", 'at', \"aren't\", 'won', \"couldn't\", 'herself', 'most', 'once', 'as', 'an', 'needn', 'her', 'on', 've', 'what', 'why', \"shan't\", 'below', 'other', 'ours', 'such', 'weren', 'in', 'just', 'those', 'or', 'couldn', 'ma', 'there', \"should've\", 'because', 'now', 'for', 'has', \"didn't\", \"that'll\", 'my', 'll', 'was', 'of', 'off', 'too', 'from', 'while', 'over', 'more', 'than', 'both', 'don', \"needn't\", 'not', 'his', 'will', 'are', 'myself', 'through', \"isn't\", 'that', 'hadn', 'above', 'y', 'been', 't', 'some', \"it's\", 'whom', 'themselves', 'she', 's', 'doing', 'under', 'he', 'm', 'so', 'any', 'were', 'aren', 'its', 'after', 'where', 'with', 'very', 'itself', 'again', 'then', 'all', 'out', 'no', 'can', 'their', 'i', 'didn', 'yourselves', 'having', 'against', \"wasn't\", 'this', 'ourselves', 'isn', 'have', 'nor', 'down', 'does', 'between', 'it', 'do', 'only', 'same', \"you're\", 'own', 'is', 'if', 'am', 'we', \"hasn't\", 'himself', 'here', 're', \"haven't\", 'should', \"you'll\", 'd', 'which', 'to', \"wouldn't\", 'doesn', 'up', 'few', 'when', 'mightn', 'hasn', 'shouldn', 'theirs', \"mightn't\", \"you'd\", 'the', \"won't\", 'during', 'further', 'haven', 'shan', 'wasn', 'yourself', \"shouldn't\", \"doesn't\", 'into', 'being'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "print(english_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "duplicate-jefferson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MacGyver', 'resolve', 'problem', 'use', 'clip', 'rubber', 'band', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens_sw = [word for word in lemmas if word not in english_stops]\n",
    "print(tokens_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-consortium",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging / POS Tag\n",
    "\n",
    "Es un tipo de clasificación en donde se clasifican los token segun la semántica. Sujeto, verbo, adverbios, adjetivos, pronombres, etc.\n",
    "\n",
    "<img src=\"Images/Tags.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "identified-nickname",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MacGyver', 'RB'), ('resolved', 'VBD'), ('the', 'DT'), ('problem', 'NN'), ('using', 'VBG'), ('only', 'RB'), ('a', 'DT'), ('clip', 'NN'), ('and', 'CC'), ('some', 'DT'), ('rubber', 'NN'), ('bands', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokens_n_tags = nltk.pos_tag(tokens)\n",
    "print(tokens_n_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-chapel",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "Separar la oración o frase en grupos según su análisis sintáctico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "considered-dinner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [('MacGyver', 'RB'), ('resolved', 'VBD'), Tree('NP', [('the', 'DT'), ('problem', 'NN')]), ('using', 'VBG'), ('only', 'RB'), Tree('NP', [('a', 'DT'), ('clip', 'NN')]), ('and', 'CC'), Tree('NP', [('some', 'DT'), ('rubber', 'NN')]), ('bands', 'NNS'), ('.', '.')])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar_np = r\"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunk_parser = nltk.RegexpParser(grammar_np)\n",
    "chunk_result = chunk_parser.parse(tokens_n_tags)\n",
    "chunk_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-gothic",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "Es una lista de palabras de referencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adjustable-ordinary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MacGyver', 'resolve', 'problem', 'use', 'clip', 'rubber', 'band', '']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "tokens_stem_1\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "bag_of_words = [re_punc.sub('',w) for w in tokens_sw]\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-injection",
   "metadata": {},
   "source": [
    "### Vectorización\n",
    "\n",
    "Codificar las palabras del texto en valores numéricos para ser procesado por el algoritmo de aprendizaje automatizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "standard-calvin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer vocabulary:\n",
      "{'macgyver': 2, 'resolve': 4, 'problem': 3, 'use': 6, 'clip': 1, 'rubber': 5, 'band': 0}\n",
      "[[1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "txt = [' '.join(bag_of_words)]\n",
    "vectorizer.fit(txt)\n",
    "vector = vectorizer.transform(txt)\n",
    "print(\"Vectorizer vocabulary:\")\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-order",
   "metadata": {},
   "source": [
    "Si se tomaran dos oracione y se procesarán, solo la primera tendría resultados positivos.\n",
    "\n",
    "1. **MacGyver** is portrayed as a non-violent **problem** solver and typically eschews the **use** of guns.His violent actions are performed in self-defense, and he takes non-fatal action when possible.\n",
    "\n",
    "2. Natural language processing is a subfield of computer science, and artificial intelligence concerned with the interactions between computers and human language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-catalog",
   "metadata": {},
   "source": [
    "## Uso de Redes Neuronales\n",
    "\n",
    "### Ejemplo 2: Clasificación de reseñas de películas usando redes neuronales con Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-playing",
   "metadata": {},
   "source": [
    "Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "backed-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-stadium",
   "metadata": {},
   "source": [
    "#### Funciones de preprocesado\n",
    "\n",
    "Cargar archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "oriental-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-benefit",
   "metadata": {},
   "source": [
    "Preprocesado del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "outstanding-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token,pos='n')\n",
    "        lemma = lemmatizer.lemmatize(lemma,pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    tokens_sw = [word for word in lemmas if word not in english_stops]\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub('',w) for w in tokens_sw]    \n",
    "    Tokens = [word for word in tokens if len(word) > 1]\n",
    "    return Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-local",
   "metadata": {},
   "source": [
    "Crear la lista de palabras o *bag of words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "economic-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(filename,vocab):\n",
    "    text = load_doc(filename)\n",
    "    clean_text = preprocess(text)\n",
    "    vocab.update(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-cincinnati",
   "metadata": {},
   "source": [
    "Leer los archivos del directorio que empiecen con 'cv9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "higher-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_in_dir(directory,vocab):\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        bag_of_words(path,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-nirvana",
   "metadata": {},
   "source": [
    "Guardar la lista de palabras o *bag of words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "photographic-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(filename,text):\n",
    "    data = '\\n'.join(text)\n",
    "    file = open(filename,'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-repeat",
   "metadata": {},
   "source": [
    "#### Funciones de procesado\n",
    "\n",
    "Leer los archivos para ser procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "english-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_to_process(directory,vocab,is_train):\n",
    "    lines = list()\n",
    "    for filename in listdir(directory):\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        text = load_doc(path)\n",
    "        words = preprocess(text)\n",
    "        line = [word for word in words if word in vocab]\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-academy",
   "metadata": {},
   "source": [
    "Acomodar el data set en reseñas positivas y negativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "proud-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab,is_train):\n",
    "    pos = read_files_to_process('movie_reviews/pos',vocab,is_train)\n",
    "    neg = read_files_to_process('movie_reviews/neg',vocab,is_train)\n",
    "    dataset = neg + pos\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return dataset,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-region",
   "metadata": {},
   "source": [
    "Tokenización del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sound-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(dataset):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(dataset)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-injury",
   "metadata": {},
   "source": [
    "Definición de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dutch-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(n_words):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50,input_shape=(n_words,),activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    plot_model(model,to_file='model.png',show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-hello",
   "metadata": {},
   "source": [
    "#### Ejecución\n",
    "\n",
    "Crear lista de *Bag of Words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "hearing-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "read_files_in_dir('movie_reviews/pos',vocab)\n",
    "read_files_in_dir('movie_reviews/neg',vocab)\n",
    "min_occurane = 2\n",
    "text = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "save_list(\"bag_of_words.txt\",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-projection",
   "metadata": {},
   "source": [
    "Procesado de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abstract-devon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                984600    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 984,651\n",
      "Trainable params: 984,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "57/57 - 2s - loss: 0.6910 - accuracy: 0.6500\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.6790 - accuracy: 0.8111\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.6577 - accuracy: 0.8628\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.6268 - accuracy: 0.8950\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.5897 - accuracy: 0.8961\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.5454 - accuracy: 0.9300\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.5013 - accuracy: 0.9217\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.4574 - accuracy: 0.9367\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.4156 - accuracy: 0.9406\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 0.3765 - accuracy: 0.9539\n",
      "Test Accuracy: 84.500003\n"
     ]
    }
   ],
   "source": [
    "vocab_filename = 'bag_of_words.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "\n",
    "xtrain,ytrain = load_clean_dataset(vocab,True)\n",
    "xtest,ytest = load_clean_dataset(vocab,False)\n",
    "tokenizer = create_tokenizer(xtrain)\n",
    "Xtrain = tokenizer.texts_to_matrix(xtrain,mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(xtest,mode='freq')\n",
    "\n",
    "Xtrain = np.array(Xtrain)\n",
    "ytrain = np.array(ytrain)\n",
    "Xtest = np.array(Xtest)\n",
    "ytest = np.array(ytest)\n",
    "\n",
    "n_words = Xtest.shape[1]\n",
    "model = define_model(n_words)\n",
    "model.fit(Xtrain,ytrain,epochs=10,verbose=2)\n",
    "loss,acc = model.evaluate(Xtest,ytest,verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-vault",
   "metadata": {},
   "source": [
    "## RNN Redes Neuronales Recurrentes \n",
    "\n",
    "La idea es retro alimentar la red con información previa. En el caso de NLP esta información previa brinda el contexto de la oración.\n",
    "\n",
    "<img src=\"Images/RNN_1.jpeg\" width=\"400\">\n",
    "\n",
    "Además se puede hacer uso del análisis semántico para pasar los *chunks* a la red.\n",
    "\n",
    "Mediante el uso de RNNs se creatron arquitecturas para la traducción de texto. Se emplea un *encoder* que mapea la secuencia de entrada a una representación abstracta. \n",
    "\n",
    "Y posterior un *decoder* crea una nueva representación abstracta teniendo en cuenta la representación del encoder.\n",
    "\n",
    "En medio de ambos bloques de *encoder* y *decoder* se encuentra un bloque de *attention* que brinda contexto de la representación abstracta del encoder a la representación abstracta del decoder.\n",
    "\n",
    "<img src=\"Images/Neural_Machine_Translation.png\" width=\"600\">\n",
    "\n",
    "Una desventaja de estos modelos es la memoria de corto plazo que poseen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-relay",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Con el concepto de *Attention Mechanism* aparecen los *Transformers* que llegaron a tener mejores rendimientos que las arquitecturas previas. Esto debido a que este modelo no sufre de las deficiencias de la memoria a corto plazo. Y más bien con suficientes recursos computacionales, cuentan con una memoria casi infita.\n",
    "\n",
    "<img src=\"Images/Transformer Model Architecture.png\" width=\"400\">\n",
    "\n",
    "### Encoder\n",
    "\n",
    "#### Input Embbeding\n",
    "Busca en una lista de palabras un factor de representación para las paabras del texto. Si las palabras son similares el factor de representación será similar.\n",
    "\n",
    "#### Position Encoding\n",
    "Como este modelo no cuenta con recurrencia como en el caso de las RNNs. A las palabras se les asigna información de posición usando funciones seno y coseno.\n",
    "\n",
    "$\\text{P.E.}(\\text{pos},2i) = \\text{sin}(\\frac{\\text{pos}}{10000^{2i}/\\text{dmodel}})$\n",
    "\n",
    "$\\text{P.E.}(\\text{pos},2i+1) = \\text{cos}(\\frac{\\text{pos}}{10000^{2i}/\\text{dmodel}})$\n",
    "\n",
    "#### Multi Head Attention\n",
    "\n",
    "<img src=\"Images/Scaled Dot-Product Attention & Multi-Head Attention.png\" width=\"600\">\n",
    "\n",
    "Este bloque cuenta con tres entradas: \n",
    "\n",
    "* **Query** ($Q$): Se puede ver como una *petición* que se hace.\n",
    "* **Key** ($K$): Son las *palabras claves* que mejor encajan para responder la *petición*\n",
    "* **Value** ($V$): Es un valor de escalamiento.\n",
    "\n",
    "##### Scaled Dot-Product Attention\n",
    "\n",
    "* Se multiplica $Q \\times K = $ *score matrix*.\n",
    "* Se escala el resultado o *score matrix* dividiendo entre la raíz de la dimensión de $Q$ y $K$: *score_matrix*$/\\sqrt{dim}$. Este escalamiento le da más estabilidad al gradiente. Los valores más altos representan las palabras más relevantes.\n",
    "\n",
    "<img src=\"Images/score_matrix.png\" width=\"200\">\n",
    "\n",
    "* Se le aplica una función *softmax* para obtener una matriz de pesos\n",
    "* Se multiplica la matriz de pesos por $V$.\n",
    "\n",
    "Después del bloque *Scaled Dot-Product Attention* se normaliza el resultado.\n",
    "\n",
    "\n",
    "### Decoder\n",
    "\n",
    "El *Decoder* funciona casi igual al *Encoder* con la diferenncia que el *Decoder* cuenta con un bloque *Multi Head Attention* que enmascara el resutado. Para el enmascaramiento se usa una matriz llamada *look ahead mask*, la cual se suma a la *score matrix*.\n",
    "\n",
    "<img src=\"Images/look_ahead_mask.png\" width=\"600\">\n",
    "\n",
    "El uso de $-\\infty$ se debe a que al posterior realizar la función *softmax*, los $-\\infty$ pasan a ceros.\n",
    "\n",
    "<img src=\"Images/softmax_look_ahead_mask.png\" width=\"600\">\n",
    "\n",
    "La razón de este enmascaramiento es para innivir al bloque *Multi Head Attention* de buscar palabras futuras en la oración."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-horizon",
   "metadata": {},
   "source": [
    "### Ejemplo 3: Clasificación de texto mediante Transmormers\n",
    "\n",
    "Importar las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "alpine-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-leone",
   "metadata": {},
   "source": [
    "Implementar el bloque de *Transformer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "narrative-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-configuration",
   "metadata": {},
   "source": [
    "Implementar la tokenización y *Position Embedding*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "supposed-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-cookbook",
   "metadata": {},
   "source": [
    "Cargar y preparar el *dataset*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "interior-skill",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/home/flz/miniconda3/envs/ML/lib/python3.9/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/flz/miniconda3/envs/ML/lib/python3.9/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-bangladesh",
   "metadata": {},
   "source": [
    "Crear el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "critical-second",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method TokenAndPositionEmbedding.call of <__main__.TokenAndPositionEmbedding object at 0x7f69dd3136d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TokenAndPositionEmbedding.call of <__main__.TokenAndPositionEmbedding object at 0x7f69dd3136d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-language",
   "metadata": {},
   "source": [
    "Entrenamiento y evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "laughing-sport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 525s 669ms/step - loss: 0.0574 - accuracy: 0.9825 - val_loss: 0.5762 - val_accuracy: 0.8428\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 563s 720ms/step - loss: 0.0345 - accuracy: 0.9905 - val_loss: 0.7359 - val_accuracy: 0.8352\n"
     ]
    }
   ],
   "source": [
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-prisoner",
   "metadata": {},
   "source": [
    "## Transformers con modelos pre-entrenados\n",
    "\n",
    "### Bidirectional Encoder Representations from Transformers (BERT) \n",
    "\n",
    "Es un modelo de aprendizaje automático basada en *Transformer* desarrollada por Google. Entrenado con 800 millones de palabras de BookCorpus y 2500 millones de palabras de artículos de wikipedia en inglés.\n",
    "\n",
    "### Generative Pre-trained Transformer 2 (GPT-2)\n",
    "\n",
    "Es un modelo de lenguaje auto regresivo de *deep learning* para la generación de lenguaje natural desarrollado por OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-colony",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "[1] [Natural language](https://en.wikipedia.org/wiki/Natural_language)\n",
    "\n",
    "[2] [What Is Natural Language Processing?](https://machinelearningmastery.com/natural-language-processing/)\n",
    "\n",
    "[3] [NLP, AI, and Machine Learning: What’s The Difference?](https://monkeylearn.com/blog/nlp-ai/)\n",
    "\n",
    "[4] [ Deep Learning for Natural Language Processing Develop Deep Learning Models for Natural Language in Python](http://ling.snu.ac.kr/class/AI_Agent/deep_learning_for_nlp.pdf)\n",
    "\n",
    "[5] [Natural Language Processing (NLP) & Text Mining Tutorial Using NLTK | NLP Training | Edureka](https://www.youtube.com/watch?v=05ONoGfmKvA&t=0s)\n",
    "\n",
    "[6] [PyCon 2020 Natural Language Processing (NLP) in Python](https://www.youtube.com/watch?v=vyOgWhwUmec)\n",
    "\n",
    "[7] [Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "\n",
    "[8] [Text classification with Transformer](https://keras.io/examples/nlp/text_classification_with_transformer/)\n",
    "\n",
    "[9] [Illustrated Guide to Transformers Neural Network: A step by step explanation](https://www.youtube.com/watch?v=4Bdc55j80l8)\n",
    "\n",
    "[10] [Write with Transformers](https://transformer.huggingface.co/doc/gpt2-large)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
